{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0589917b"
      },
      "source": [
        "# üìö Grammar Correction Model ‚úçÔ∏è\n",
        "\n",
        "This notebook demonstrates how to fine-tune a T5 model for grammar correction using the C4_200M dataset.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1.  **Setup and Installation:** Install necessary libraries (Hugging Face Transformers, PyTorch, nltk, kagglehub, pandas).\n",
        "2.  **Data Download:** Download the C4_200M dataset using `kagglehub`.\n",
        "3.  **Data Loading and Preprocessing:** Load a subset of the dataset, handle missing values, and format the input and output columns.\n",
        "4.  **Model and Tokenizer Initialization:** Load a pre-trained T5 model and tokenizer.\n",
        "5.  **Dataset Preparation:** Create a custom PyTorch `Dataset` for the grammar correction task and split the data into training and validation sets.\n",
        "6.  **Training:** Configure training arguments and train the T5 model on the prepared dataset.\n",
        "7.  **Saving the Model:** Save the fine-tuned model and tokenizer to a local directory.\n",
        "8.  **Testing:** Test the fine-tuned model with example sentences to see its grammar correction capabilities.\n",
        "9.  **Download Model:** Zip and download the trained model for future use.\n",
        "\n",
        "Let's get started! ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18207921"
      },
      "source": [
        "## ‚öôÔ∏è Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ecda2a"
      },
      "source": [
        "!pip install torch transformers nltk pandas kagglehub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e5a28d4"
      },
      "source": [
        "## ‚ú® Upgrade Transformers and Accelerate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "093abeb8"
      },
      "source": [
        "!pip install --upgrade transformers accelerate datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5415b030"
      },
      "source": [
        "## ‚¨áÔ∏è Download NLTK Data and Check CUDA Availability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27da44b8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18e7d6f6"
      },
      "source": [
        "## ‚¨áÔ∏è Download Additional NLTK Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "133c7afa"
      },
      "source": [
        " import nltk\n",
        " nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b815f48e"
      },
      "source": [
        "## üíæ Download C4_200M Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16e92e9e"
      },
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download C4_200M dataset\n",
        "path = kagglehub.dataset_download(\"dariocioni/c4200m\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# List files in the downloaded directory to verify\n",
        "files = os.listdir(path)\n",
        "print(\"Downloaded files:\", files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e1dafa3"
      },
      "source": [
        "## üìä Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efdc59be"
      },
      "source": [
        "print(\"--- Loading and Preprocessing Data ---\")\n",
        "# Path to your already downloaded dataset\n",
        "data_path = \"/root/.cache/kagglehub/datasets/dariocioni/c4200m/versions/4/\"\n",
        "tsv_file_name = \"C4_200M.tsv-00000-of-00010\"\n",
        "tsv_path = os.path.join(data_path, tsv_file_name)\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(tsv_path, sep='\\t', nrows=NUM_ROWS_TO_LOAD, names=['input', 'output'], header=None, on_bad_lines='skip')\n",
        "    df.dropna(inplace=True)\n",
        "    df['input'] = \"grammar: \" + df['input'].astype(str)\n",
        "    df['output'] = df['output'].astype(str)\n",
        "    print(f\"Dataset loaded successfully. Size: {len(df)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Could not find the dataset file at {tsv_path}. Please check the path.\")\n",
        "    exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd341899"
      },
      "source": [
        "## ‚öôÔ∏è Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b415a05f"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "# Configuration parameters\n",
        "MODEL_NAME = \"t5-small\"\n",
        "OUTPUT_DIR = \"./t5-grammar-correction-model\"\n",
        "NUM_ROWS_TO_LOAD = 100000\n",
        "MAX_INPUT_LENGTH = 128\n",
        "MAX_TARGET_LENGTH = 128\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 3e-4\n",
        "EPOCHS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f46b43f7"
      },
      "source": [
        "## üèóÔ∏è Define Grammar Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fb28430"
      },
      "source": [
        "class GrammarDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_input_len, max_target_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.max_input_len = max_input_len\n",
        "        self.max_target_len = max_target_len\n",
        "        self.inputs = self.data['input'].tolist()\n",
        "        self.targets = self.data['output'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_text = self.inputs[index]\n",
        "        target_text = self.targets[index]\n",
        "\n",
        "        source = self.tokenizer(source_text, max_length=self.max_input_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        target = self.tokenizer(target_text, max_length=self.max_target_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        input_ids = source[\"input_ids\"].squeeze()\n",
        "        attention_mask = source[\"attention_mask\"].squeeze()\n",
        "        labels = target[\"input_ids\"].squeeze()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a071d19e"
      },
      "source": [
        "## ü§ñ Initialize Model and Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c193e02"
      },
      "source": [
        "print(\"\\n--- Initializing Model and Preparing Datasets ---\")\n",
        "tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "train_dataset = GrammarDataset(train_df, tokenizer, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH)\n",
        "val_dataset = GrammarDataset(val_df, tokenizer, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH)\n",
        "print(f\"Training set size: {len(train_dataset)}, Validation set size: {len(val_dataset)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "258f288c"
      },
      "source": [
        "## üèãÔ∏è Configure and Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdd3806b"
      },
      "source": [
        "print(\"\\n--- Configuring and Starting Training ---\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b75a6fd5"
      },
      "source": [
        "## üíæ Save the Final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6beda18f"
      },
      "source": [
        "# --- Save the Final Model ---\n",
        "print(f\"\\n--- Saving model to {OUTPUT_DIR} ---\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Model saved successfully. ‚úÖ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401e6ea6"
      },
      "source": [
        "## ‚úÖ Test the Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5e23ac2"
      },
      "source": [
        "# --- Test the Fine-Tuned Model ---\n",
        "print(\"\\n--- Testing the fine-tuned model ---\")\n",
        "\n",
        "# Load the model and tokenizer from your output directory\n",
        "trained_model = T5ForConditionalGeneration.from_pretrained(OUTPUT_DIR)\n",
        "trained_tokenizer = T5TokenizerFast.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Set up the device (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "trained_model.to(device)\n",
        "\n",
        "def correct_grammar(text: str):\n",
        "    \"\"\"Uses the fine-tuned model to correct a given sentence.\"\"\"\n",
        "    # Prepare the input text with the \"grammar:\" prefix\n",
        "    input_text = \"grammar: \" + text\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = trained_tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Move tensors to the correct device\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "    # Generate the corrected output\n",
        "    outputs = trained_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=MAX_TARGET_LENGTH,\n",
        "        num_beams=4,  # Use beam search for better results\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the generated ids to a string\n",
        "    corrected_text = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_text\n",
        "\n",
        "# Example sentences to test\n",
        "test_sentences = [\n",
        "    \"The cats plays in the garden.\",\n",
        "    \"He is a honest man.\",\n",
        "    \"I will went to the store tomorrow.\",\n",
        "    \"This book is more better than that one.\",\n",
        "    \"Me and him are going to the movies.\",\n",
        "    \"She is good in math.\",\n",
        "    \"There is many reasons to be happy.\",\n",
        "    \"He don't know the answer.\"\n",
        "]\n",
        "\n",
        "# Run the tests and print the results\n",
        "for sentence in test_sentences:\n",
        "    correction = correct_grammar(sentence)\n",
        "    print(f\"Original:   '{sentence}'\")\n",
        "    print(f\"Corrected:  '{correction}'\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7199542"
      },
      "source": [
        "test_sentences = [\n",
        "    \"He is a honest man.\",\n",
        "    \"I will went to the store tomorrow.\",\n",
        "    \"This book is more better than that one.\",\n",
        "    \"There is many reasons to be happy.\",\n",
        "    \"He don't know the answer.\"\n",
        "]\n",
        "# Run the tests and print the results\n",
        "for sentence in test_sentences:\n",
        "    correction = correct_grammar(sentence)\n",
        "    print(f\"Original:   '{sentence}'\")\n",
        "    print(f\"Corrected:  '{correction}'\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "585b77c8"
      },
      "source": [
        "## üì¶ Download the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adf52146"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Zip the entire model directory\n",
        "!zip -r /content/model.zip /content/t5-grammar-correction-model\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/model.zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}